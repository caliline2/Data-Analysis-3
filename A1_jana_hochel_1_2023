---
title: "DA3 Assigment 1"
author: "Jana Hochel"
date: "2023-01-24"
output:
  pdf_document: default
  html_document: default
  word_document: default
---


### Inspired by Gabor BEKES and  Gabor KEZDI 2021 code 


```{r include=FALSE}
#Clear memory
rm(list=ls())

# import libraries
library(tidyverse)
library(modelsummary)
library(fixest)
library(lmtest)
library(sandwich)
#library(haven)
library(stargazer)
library(caret)
library(grid)
library(kableExtra)

################################################################################

# DATA IMPORT

data <- read.csv( 'https://osf.io/4ay9x/download', stringsAsFactors = TRUE)


### Selecting subset Finance and IT
data <- subset(data, data$occ2012>1000)
data<- subset(data,data$occ2012<1551)
```

# 0.Data Cleaning
I have selected occupation IT and Engineering (excl. Architecture, Civil and Urban Eng.) to compare hardware and software roles in tech. All the other entries were dropped to improve performance.

Second, I have created dummy variables for IT Occupation (Y/N), married (Y/N), have a children(Y/N), and university degree(Y/N).

The earnings were changed to earnings per hour as per the assigment. Third, I have created dummy variables for IT Occupation (Y/N), married (Y/N), have a children(Y/N), and university degree(Y/N). 

Fourth step was to try out the models.

## 2.Compare model performance of these models 
### (a) RMSE in the full sample, 
### (2) cross-validated RMSE and
### (c) BIC in the full sample. 

The lower the RMSE and BIC the better. RMSE penalizes for error. BIC penalizes for complexity taking into consideration the explanatory power of the model.

The most complicated Model no. 9 (reg4) displays the highest R-square but also lowest RMSE, cross-validated RMSE, and BIC. That means we would select this model for any future modelling. 

### 3.	Discuss the relationship between model complexity and performance.

Since our models are fairly simple, the RMSE decreases with more complexity. THis may change eventually as we would add more interactions and variables. The R-square keeps increasing with complexity but there would be a cap to this after certain point once we start overfitting.

## Scroll down for the results.

\newpage


```{r  include=FALSE}


# R adding a column to dataframe based on values in other columns:
data <- data %>% 
  mutate(IT_Occupation = if_else(data$occ2012 < 1300, 1, 0))

data

# cylinders
data <- data %>%
  mutate(highereducation = ifelse(grade92>42,1,0))

# cylinders
data <- data %>%
  mutate(children = ifelse(ownchild>0,1,0))

# cylinders
data <- data %>%
  mutate(married = ifelse(marital<3,1,0))

# Decriptive Statistics

ls(data)

#kableExtra::kable(sapply(data1, class))

datasummary_skim(data)

# check the datatable
glimpse( data )

data <- data %>%
  mutate(
    EarningPH =  earnwke/uhours)



# SAMPLE DESIGN
unique(data$EarningPH)
unique(data$age)
unique(data$weight)
unique(data$uhours)
unique(data$grade92)
unique(data$marital)
unique(data$ownchild)
unique(data$IT_Occupation)

# check frequency by Occupation type
data %>%
  group_by(IT_Occupation) %>%
  dplyr::summarize(frequency=n()) %>%
  mutate(percent = frequency / sum(frequency)*100,
         cumulative_percent = cumsum(frequency)/sum(frequency)*100)

#61% of observations are IT-sector employees and 39% are engineers.

unique(data$EarningPH)
unique(data$age)
unique(data$uhours)
unique(data$grade92)
unique(data$marital)
unique(data$ownchild)
unique(data$IT_Occupation)

data %>%
  group_by(IT_Occupation) %>%
  summarise(
    frequency=n(),
    P1 = quantile(EarningPH, 0.01),
    D1 = quantile(EarningPH,0.1),
    Q1 = quantile(EarningPH, 0.25),
    Me = quantile(EarningPH, 0.5),
    Q3 = quantile(EarningPH, 0.75),
    D9 = quantile(EarningPH, 0.75),
    D9 = quantile(EarningPH, 0.9),
    P99 = quantile(EarningPH, 0.99),
    max = max(EarningPH))

Hmisc::describe(data$EarningPH)


# check frequency by vehicle condition
data %>%
  group_by(sex) %>%
  dplyr::summarize(frequency=n()) %>%
  mutate(percent = frequency / sum(frequency)*100,
         cumulative_percent = cumsum(frequency)/sum(frequency)*100)

# drop vehicles in fair and new condition, trucks
#data <- data %>% filter(!EarningPH %in% c("new", "fair"))

# check frequency by transmission
data %>%
  group_by(ownchild) %>%
  dplyr::summarize(frequency=n()) %>%
  mutate(percent = frequency / sum(frequency)*100,
         cumulative_percent = cumsum(frequency)/sum(frequency)*100)


data %>%
  group_by(occ2012) %>%
  dplyr::summarize(frequency=n()) %>%
  mutate(percent = frequency / sum(frequency)*100,
         cumulative_percent = cumsum(frequency)/sum(frequency)*100)


################################################################################

# DATA GENERATION & DESCRIPTIVES

data %>%
  group_by(highereducation) %>%
  dplyr::summarize(frequency=n()) %>%
  mutate(percent = frequency / sum(frequency)*100,
         cumulative_percent = cumsum(frequency)/sum(frequency)*100)


datasummary(highereducation + as.factor( highereducation) ~ N + Percent() , data = data )


# age: quadratic, cubic
data <- data %>%
  mutate(agesq = age^2,
         agecu = age^3,
         ownchildsq = ownchild^2,
         ownchildcu = ownchild^3,
         uhourssq = uhours^2,
         uhourscu= uhours^3,
         grade92sq = grade92^2,
         grade92cu = grade92^3)


# save workfile
#write.csv(data, paste0(data_out, "usedcars_work.csv"), row.names = F)

###############################################################################

#data <- read.csv(paste0(data_out, "usedcars_work.csv"), stringsAsFactors = FALSE)

data %>%
  group_by(IT_Occupation) %>%
  dplyr::summarize(frequency=n(), mean=mean(EarningPH))

t.test(EarningPH~IT_Occupation,data=data)

#The t-value is small and P-value is higher than 0.05 thus we fail to reject the 0 hypothesis although the mean for IT sector is seemingly higher.

t.test(EarningPH~highereducation,data=data)
#The t-value is large in absolute value. Similarly P<0.05. Thus, we can reject the H0 and assume the mean is significantly higher for people with higher education degree.

t.test(EarningPH~children,data=data)
#The t-value is large in absolute value. Similarly P<0.05. Thus, we can reject the H0 and assume the mean is significantly higher for people with children.

t.test(EarningPH~married,data=data)
#The t-value is large in absolute value. Similarly P<0.05. Thus, we can reject the H0 and assume the mean is significantly higher for people that are married (not separated/widowed).


# P is probability that we reject null hypothesis (probably we make mistake - incorreclty rejecting the null hypothesis).

# Frequency tables

datasummary( as.factor( IT_Occupation ) * EarningPH ~ N + Mean , data = data )

datasummary( as.factor(highereducation) * EarningPH ~ N + Mean , data = data )

datasummary( as.factor( children ) * EarningPH ~ N + Mean , data = data )


# data summary
#datasummary( age + odometer + LE + XLE + SE + cond_likenew + cond_excellent + cond_good + cylind6 + age ~
#              Mean + Median + Min + Max + P25 + P75 + N , data = data )

# Histograms not in textbook
# price
hist(data$EarningPH, xlim = c(0, 120),xlab = "Earnings per hour",labels = scales::percent_format(1))
# labels = scales::percent_format(accuracy = 1)


###############################################################################
# REGRESSION ANALYSIS


# lowess
Ch13_p_age_lowess_R <- ggplot(data = data, aes(x=EarningPH, y=ownchild)) +
  geom_point( color = "aquamarine2", size = 1,  shape = 16, alpha = 0.8, show.legend=F, na.rm = TRUE) + 
  geom_smooth(method="loess", se=F, colour="aquamarine2", size=1, span=0.9) +
  labs(x = "Earnings per hour",y = "Number of children") +
  
  expand_limits(x = 0.01, y = 0.01) +
  scale_y_continuous(expand = c(0.01,0.01),limits = c(0,10), breaks = seq(0,20, 1)) +
  scale_x_continuous(expand = c(0.01,0.01),limits = c(0,200), breaks = seq(0,300, 50))
Ch13_p_age_lowess_R

Ch13_p_age_lowess_R <- ggplot(data = data, aes(x=EarningPH, y=age)) +
  geom_point( color = "aquamarine2", size = 1,  shape = 16, alpha = 0.8, show.legend=F, na.rm = TRUE) + 
  geom_smooth(method="loess", se=F, colour="aquamarine2", size=1, span=0.9) +
  labs(x = "Earnings per hour",y = "Age") +
  expand_limits(x = 0.01, y = 0.01) +
  scale_y_continuous(expand = c(0.01,0.01),limits = c(0,65), breaks = seq(0,100, 5)) +
  scale_x_continuous(expand = c(0.01,0.01),limits = c(0,200), breaks = seq(0,300, 50))
Ch13_p_age_lowess_R




###################################
# Linear regressions

# Model 1: Linear regression on age to double check the most important predictors
model1 <- as.formula(EarningPH ~ age +agesq)
class(model1)
model1

model2 <- as.formula(EarningPH~IT_Occupation)
class(model2)
model2

model3 <- as.formula(EarningPH~highereducation)
class(model3)
model3

model4 <- as.formula(EarningPH~ownchild)
class(model4)
model4

model5 <- as.formula(EarningPH~sex)
class(model5)
model5

model10 <- as.formula(EarningPH~married)
class(model10)
model10

# Models 2-5: Multiple linear regressions
# note: condition - missing will be baseline for regs

```

# 1.The 4 models:
```{r  include=TRUE, message=FALSE}

model6 <- as.formula(EarningPH~IT_Occupation+age+married+sex+ownchild+highereducation)
model7 <- as.formula(EarningPH~IT_Occupation+agesq+married+sex+ownchild+highereducation)
model8 <- as.formula(EarningPH~IT_Occupation+agecu+married+sex+ownchild+highereducation)
model9 <- as.formula(EarningPH~IT_Occupation+log(age)+married+sex+ownchild+highereducation+ IT_Occupation*highereducation)

```


```{r  include=FALSE, echo=FALSE}


# Running simple OLS
reg0a <- feols(model1, data=data, vcov = 'hetero')
reg0b <- feols(model2, data=data, vcov = 'hetero')
reg0c <- feols(model3, data=data, vcov = 'hetero')
reg0d <- feols(model4, data=data, vcov = 'hetero')
reg0e <- feols(model5, data=data, vcov = 'hetero')
reg0f <- feols(model10, data=data, vcov = 'hetero')
reg1 <- feols(model6, data=data, vcov = 'hetero')
reg2 <- feols(model7, data=data, vcov = 'hetero')
reg3 <- feols(model8, data=data, vcov = 'hetero')
reg4 <- feols(model9, data=data, vcov = 'hetero')



#BIC = k*ln(n) - 2*2*(max value of the likelihood )
reg0a
reg0b
reg0c
reg0d
reg0e
reg0f
reg1
reg2
reg3
reg4

# evaluation of the models
fitstat_register("k", function(x){length( x$coefficients ) - 1}, "No. Variables")
```
## 3. Cross-validation RMSE

```{r  include=TRUE, echo=FALSE, message=FALSE}
kable(cv_mat)

```


```{r  include=FALSE}

stargazer(cv_mat, summary = F, digits=0, float=F,sep="") #out=paste(output,"ch13-table-5-cvmat.tex",)
stargazer(cv_mat, summary = F, digits=0, float=F, type="text",sep="")  #out=paste(output,"ch13-table-5-cvmat.txt",sep=""))

```


```{r  include=FALSE}

data <- data %>% dplyr::select(EarningPH,IT_Occupation,age,marital,sex,ownchild,highereducation)

# Add new observation
new <- tibble(IT_Occupation=1,age=25,married=1,sex=1,ownchild=0,highereducation=1)

# cylinders
data <- data %>%
  mutate(married = ifelse(marital<3,1,0))

model6 <- as.formula(EarningPH~IT_Occupation+age+married+sex+ownchild+highereducation)
class(model6)
model7 <- as.formula(EarningPH~IT_Occupation+agesq+married+sex+ownchild+highereducation)
model8 <- as.formula(EarningPH~IT_Occupation+agecu+married+sex+ownchild+highereducation)
model9 <- as.formula(EarningPH~IT_Occupation+log(age)+married+sex+ownchild+highereducation)
class(model9)

# Predict price with only 2 predictors (Model1)
pred1 <- feols(model6, data=data, vcov = 'hetero')
summary(pred1)

#View RMSE
pred1

# Standard errors of residuals
p1 <- predict(pred1, data)
resid_p1 <- p1-data$EarningPH
summary(resid_p1)


# predict value for newly added obs
pred1_new <- predict(pred1, newdata = new ,se.fit = TRUE, interval = "prediction")
p1<- pred1_new$fit
p1

# Predict price with all predictors (Model9)
pred9 <- feols(model9, data=data,vcov = 'hetero')
pred9

p9<- pred9$fit
p9

# Standard errors of residuals
p9 <- predict(pred9, data)
resid_p9 <- p9-data$EarningPH
summary(resid_p9)

# predict value for newly added obs
pred9_new <- predict(pred9, newdata = new, se.fit = TRUE, interval = "prediction")
p9<- pred9_new$fit
pred9_new 

#get model rmse
data$p9a <- predict( pred9, data)
data$p9a

log_na <- is.na( data$p9a )
log_na

rmse9 <- RMSE(data$p9a[!log_na],data$price[!log_na])

```

```{r  include=FALSE}
kable(rmse9)

```


```{r  include=FALSE}

# Result summary
sum1 <- cbind(t(pred1_new[,c(1,3,4)]), t(pred9_new[,c(1,3,4)]))
colnames(sum1) <- c('Model9', 'Model6')
rownames(sum1) <- c('Predicted', 'PI_low (95%)', 'PI_high (95%)')

```

## 4. Predictions
#The log was disregarded as it was not part of the assigment.
```{r  include=TRUE, echo=FALSE, message=FALSE}
kable(sum1)
```


```{r  include=FALSE, echo=FALSE, message=FALSE}
kable(stargazer(sum1, summary = F, digits=0, float=F,sep="")) #out=paste(output,"ch13-table-3-pred-new.tex",sep=""))
kable(stargazer(sum1, summary = F, digits=0, float=F, type="text", sep="")) #out=paste(output,"ch13-table-3-pred-new.txt",sep=""))
# old name: Ch13_pred_R.txt

# prediction with 60% certainty (probability)

# summary of predictions and PI 60% version
# predict value for newly added obs

```


```{r  include=FALSE}
pred1_new80 <- predict(pred1, newdata = new, se.fit=TRUE, interval = "prediction", level=0.6)
p180<- pred1_new80$fit
p180

pred9_new80 <- predict(pred9, newdata = new,se.fit = TRUE, interval = "prediction", level=0.6)
p380<- pred9_new80$fit
p380

# Result summary
sum2 <- cbind(t(pred1_new80[,c(1,3,4)]), t(pred9_new80[,c(1,3,4)]))
colnames(sum2) <- c('Model9', 'Model6')
rownames(sum2) <- c('Predicted', 'PI_low (60%)', 'PI_high (60%)')

```

```{r  include=TRUE, echo=FALSE, message=FALSE}
kable(sum2)

```


```{r  include=FALSE}

kable(stargazer(sum2, summary = F, digits=0, float=F,sep="")) #out=paste(output,"ch13-table-3-pred-new80.tex",sep=""))
kable(stargazer(sum2, summary = F, digits=0, float=F, type="text",sep="")) #out=paste(output,"ch13-table-3-pred-new80.txt",sep=""))
```





